{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entrenar nuestros modelos de machine learning debemos crear un archivo de ingeniería de características.  \n",
    "\n",
    "\n",
    "\n",
    "Text Cleaning and Preparation: cleaning of special characters, downcasing, punctuation signs. possessive pronouns and stop words removal and lemmatization.\n",
    "Label coding: creation of a dictionary to map each category to a code.\n",
    "Train-test split: to test the models on unseen data.\n",
    "Text representation: use of TF-IDF scores to represent text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from emoji import UNICODE_EMOJI\n",
    "from nltk import bigrams \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4384: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4385: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Esperanza/Desktop/UOC/TFM/python/00_Dataset/clean_sentiment_labels_completo.csv\", sep=';')\n",
    "df.head()\n",
    "df.is_copy = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los datos\n",
    "Una limpieza de los datos ha sido realizada tanto en el EDA como en la extracción. Sin embargo, aún hay algunos detalles que debemos depurar para poder hacer un buen análisis sobre nuestros datos.  \n",
    "\n",
    "Es  habitual la supresión de signos de puntuación pero en este caso nos ayudan a identificar la intensidad de los comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supresión de las columnas no necesarias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La etiqueta, cabrón</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hasta que un cabron trata a la lacra cómo se d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La edad no la pones eh cabrón</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eres más guiri que los irlandeses de aquí cabr...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  num_may_esc  \\\n",
       "0                                La etiqueta, cabrón          0.0   \n",
       "1  Hasta que un cabron trata a la lacra cómo se d...          0.0   \n",
       "2                      La edad no la pones eh cabrón          0.0   \n",
       "3      Y de un cabron solo puedes obtener cabronadas          0.0   \n",
       "4  Eres más guiri que los irlandeses de aquí cabr...          0.1   \n",
       "\n",
       "   num_insultos  score_emoji_tox  emphasize  toxico  \n",
       "0             1              0.0        0.0       1  \n",
       "1             1              0.0        0.0       1  \n",
       "2             1              0.0        1.0       1  \n",
       "3             1              0.0        1.0       1  \n",
       "4             1              0.0        1.1       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng=df[['texto', 'num_may_esc','num_insultos','score_emoji_tox','emphasize','toxico']]\n",
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supresión de abreviaturas en el texto y conservión a minúsculas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario desde el csv que hemos creado\n",
    "path_ab='C:/Users/Esperanza/Desktop/UOC/TFM/python/00_Dataset/abreviaturas.csv'\n",
    "abrev = pd.read_csv(path_ab, sep=';')\n",
    "diccionario_abr=abrev.set_index('abreviatura').T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "una parguela que es famosa xq es influenser\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'una parguela que es famosa porque es influenser'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creación de una función en el que palabras como xa se convierten en para\n",
    "def delete_abrev(string):\n",
    "    words=string.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in diccionario_abr.keys():\n",
    "            words[i] = diccionario_abr[words[i]]['significado'].lower()\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "\n",
    "#Comprobación del funcionamiento\n",
    "aux=df_eng.iloc[48]['texto']\n",
    "print(aux)\n",
    "delete_abrev(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4384: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4385: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La etiqueta, cabrón</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hasta que un cabron trata a la lacra cómo se d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La edad no la pones eh cabrón</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eres más guiri que los irlandeses de aquí cabr...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  num_may_esc  \\\n",
       "0                                La etiqueta, cabrón          0.0   \n",
       "1  Hasta que un cabron trata a la lacra cómo se d...          0.0   \n",
       "2                      La edad no la pones eh cabrón          0.0   \n",
       "3      Y de un cabron solo puedes obtener cabronadas          0.0   \n",
       "4  Eres más guiri que los irlandeses de aquí cabr...          0.1   \n",
       "\n",
       "   num_insultos  score_emoji_tox  emphasize  toxico  \n",
       "0             1              0.0        0.0       1  \n",
       "1             1              0.0        0.0       1  \n",
       "2             1              0.0        1.0       1  \n",
       "3             1              0.0        1.0       1  \n",
       "4             1              0.0        1.1       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sustitución de las abreviaciones en el dataset original\n",
    "df_eng.loc[:,'texto']=df_eng.loc[:,'texto'].map(lambda x: delete_abrev(x))\n",
    "df_eng.is_copy = False\n",
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenización quitando las stopwords y los signos de puntuación**\n",
    "\n",
    "No siempre se han de quitar todas las stopwords ya que algunas como no si tienen peso en el algoritmo. En este caso eliminamos no del conjunto de palabras vacías y añadimos los signos de puntuación por tenerlos ya en cuenta en la etiqueta emphasize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cabron', 'problema', 'solucion']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = stopwords.words('spanish')\n",
    "stopWords.remove('no')\n",
    "\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'texto'].map(lambda texto: texto.replace('.',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('?',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('!',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('¿',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace(',',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('“',''))\n",
    "#Supresión de las stopword en castellano\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].apply(lambda frase: [word.lower() for word in frase.split(' ') if word.lower() not in stopWords])\n",
    "\n",
    "df_eng.loc[6,'filtered_words']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmanization**\n",
    "\n",
    "Lemmatización es el proceso de convertir una palabra a su forma básica. La diferencia entre stemming y lemmatization es que la lematización considera el contexto y convierte la palabra a su forma básica significativa, mientras que stemming simplemente elimina los últimos caracteres, lo que a menudo conduce a significados incorrectos y errores de ortografía.\n",
    "\n",
    "fuente: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'estudi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prueba de funcionamiento\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stemmer.stem(\"estudiando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estoy', 'estudiar', 'comer', 'funcionar', 'lemma', 'en', 'español', 'y', 'comer', 'no', 'funcionar', '.', 'Pero', 'que', 'decir', 'zampabollos']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text1 = \"Estoy estudiando como funciona lemma en español y como no funciona.Pero que dices zampabollos \"\n",
    "res=[]\n",
    "for token in nlp(text1):\n",
    "    res.append(token.lemma_)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de la función que extrae el origen de la palabra\n",
    "def extraccion_lemma(text): \n",
    "    str1 = ' '.join(str(e) for e in text)\n",
    "    res=[]\n",
    "    for word in nlp(str1):\n",
    "        res.append(word.lemma_)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la raiz no es tomada correctamente y no proporciona valor. Pero el lemma funciona suficientemente bien en castellano.  \n",
    "\n",
    "Algunos bugs como los presentados en https://github.com/explosion/spaCy/issues/2710 siguen sin ser solucionados. Palabras como 'estoy' no son traducidas a estar, 'como' es traducido por comer,.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La etiqueta, cabrón</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[etiquetar, cabrón]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hasta que un cabron trata a la lacra cómo se d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[cabron, tratar, lacrar, cómo, deber, indignar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La edad no la pones eh cabrón</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[edad, no, poner, eh, cabrón]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[cabron, solo, poder, obtener, cabronada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eres más guiri que los irlandeses de aquí cabr...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>[guiri, irlandés, aquí, cabrón, xd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  num_may_esc  \\\n",
       "0                                La etiqueta, cabrón          0.0   \n",
       "1  Hasta que un cabron trata a la lacra cómo se d...          0.0   \n",
       "2                      La edad no la pones eh cabrón          0.0   \n",
       "3      Y de un cabron solo puedes obtener cabronadas          0.0   \n",
       "4  Eres más guiri que los irlandeses de aquí cabr...          0.1   \n",
       "\n",
       "   num_insultos  score_emoji_tox  emphasize  toxico  \\\n",
       "0             1              0.0        0.0       1   \n",
       "1             1              0.0        0.0       1   \n",
       "2             1              0.0        1.0       1   \n",
       "3             1              0.0        1.0       1   \n",
       "4             1              0.0        1.1       1   \n",
       "\n",
       "                                    filtered_words  \n",
       "0                              [etiquetar, cabrón]  \n",
       "1  [cabron, tratar, lacrar, cómo, deber, indignar]  \n",
       "2                    [edad, no, poner, eh, cabrón]  \n",
       "3        [cabron, solo, poder, obtener, cabronada]  \n",
       "4              [guiri, irlandés, aquí, cabrón, xd]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda text: extraccion_lemma(text) )\n",
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividiendo el dataset en train, dev and test sets \n",
    "Pasos a tener en cuenta: \n",
    "- dev y test deben proceder de la misma distribución y se debe tomar de forma aleatoria del total del corpus. \n",
    "- dev y test deben tener el mismo tamaño. En nuestro caso dividiremos en train 70%, dev 15% y test 15%.\n",
    "- Se quiere tener unos datos no sesgados a la hora de realizar el algoritmo, para ello se cambia el orden de las filas pero inicializando una semilla que fije los resultados dependientes de una variable aleatoria. \n",
    "\n",
    "\n",
    "fuente: https://www.coursera.org/learn/machine-learning-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>Esperemos que el Big Data de el visto bueno pa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[esperar, big, datar, vestir, bueno, pagar, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Este pobre payaso ya no sabe donde esta el nor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[pobre, payaso, no, saber, norte, sur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>MB98 Seguro que y luego unas risas, esas respu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[mb98, seguro, luego, uno, risa, respuesta, gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Sigan peleando, la gente de a pie esta jodida ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[seguir, pelear, gente, pie, joder, desacelera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Casa Rural en Sariñena, Los Monegros, en Huesc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[casar, rural, sariñena, monegros, huesca, 12,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 texto  num_may_esc  \\\n",
       "646  Esperemos que el Big Data de el visto bueno pa...          0.0   \n",
       "336  Este pobre payaso ya no sabe donde esta el nor...          0.0   \n",
       "63   MB98 Seguro que y luego unas risas, esas respu...          0.0   \n",
       "367  Sigan peleando, la gente de a pie esta jodida ...          0.0   \n",
       "101  Casa Rural en Sariñena, Los Monegros, en Huesc...          0.0   \n",
       "\n",
       "     num_insultos  score_emoji_tox  emphasize  toxico  \\\n",
       "646             0              0.0        0.0       0   \n",
       "336             0              0.0        1.0       1   \n",
       "63              0              0.0        1.0       1   \n",
       "367             1              0.0        1.0       1   \n",
       "101             0              0.0        1.0       0   \n",
       "\n",
       "                                        filtered_words  \n",
       "646  [esperar, big, datar, vestir, bueno, pagar, cl...  \n",
       "336             [pobre, payaso, no, saber, norte, sur]  \n",
       "63   [mb98, seguro, luego, uno, risa, respuesta, gi...  \n",
       "367  [seguir, pelear, gente, pie, joder, desacelera...  \n",
       "101  [casar, rural, sariñena, monegros, huesca, 12,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmod=df_eng.reindex(np.random.RandomState(seed=42).permutation(df_eng.index))\n",
    "dfmod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542,) (542,)\n",
      "(136,) (136,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfmod['filtered_words'], dfmod['toxico'], test_size=0.2, random_state=42)\n",
    "#everytime you run it without specifying random_state, you will get a different result\n",
    "print(x_train.shape, y_train.shape) \n",
    "print( x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvamento de los datos\n",
    "\n",
    "Pickle es utilizado cuando se tiene un conjunto de datos grande y se carga ese conjunto de datos masivos en la memoria cada vez que ejecuta el programa.Almacenarlo y recuperarlo será mucho más rápido, del orden de 50 - 100x, a veces mucho más dependiendo del tamaño.\n",
    "\n",
    "fuente: https://pythonprogramming.net/python-pickle-module-save-objects-serialization/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamiento del df del modelo\n",
    "pickle_out = open(\"dfmodelo.pickle\",\"wb\")\n",
    "pickle.dump(dfmod, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamiento del x_train\n",
    "pickle_out = open(\"x_train.pickle\",\"wb\")\n",
    "pickle.dump(x_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamiento del x_test\n",
    "pickle_out = open(\"x_test.pickle\",\"wb\")\n",
    "pickle.dump(x_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamiento del y_train\n",
    "pickle_out = open(\"y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamiento del y_test\n",
    "pickle_out = open(\"y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
