{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entrenar nuestros modelos de machine learning debemos crear un archivo de ingenier√≠a de caracter√≠sticas.  \n",
    "\n",
    "\n",
    "\n",
    "Text Cleaning and Preparation: cleaning of special characters, downcasing, punctuation signs. possessive pronouns and stop words removal and lemmatization.\n",
    "Label coding: creation of a dictionary to map each category to a code.\n",
    "Train-test split: to test the models on unseen data.\n",
    "Text representation: use of TF-IDF scores to represent text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la ingenier√≠a de caracter√≠sticas es el proceso de usar el conocimiento de dominio de los datos para crear caracter√≠sticas que hacen que los algoritmos de aprendizaje autom√°tico funcionen. Es como un arte, ya que requiere conocimiento del dominio y puede ser dif√≠cil crear caracter√≠sticas, pero puede ser fruct√≠fero para el algoritmo ML para predecir los resultados, ya que pueden estar relacionados con la predicci√≥n.\n",
    "\n",
    "fuente: https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaci√≥n de las librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from emoji import UNICODE_EMOJI\n",
    "from nltk import bigrams \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4384: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4385: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../00_Dataset/clean_sentiment_labels_completo.csv\", sep=';')\n",
    "df.head()\n",
    "df.is_copy = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los datos\n",
    "Una limpieza de los datos ha sido realizada tanto en el EDA como en la extracci√≥n. Sin embargo, a√∫n hay algunos detalles que debemos depurar para poder hacer un buen an√°lisis sobre nuestros datos.  \n",
    "\n",
    "Es  habitual la supresi√≥n de signos de puntuaci√≥n pero en este caso nos ayudan a identificar la intensidad de los comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supresi√≥n de las columnas no necesarias***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>toxico</th>\n",
       "      <th>num_admir</th>\n",
       "      <th>num_may</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>emojis</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>dir_23ps</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>series_token2</th>\n",
       "      <th>series_token</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>la etiqueta, cabron</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>la etiqueta, cabron</td>\n",
       "      <td>['la', 'etiqueta', ',', 'cabron']</td>\n",
       "      <td>['etiqueta,', 'cabron']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasta que un cabron trata a la lacra como se d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hasta que un cabron trata a la lacra como se d...</td>\n",
       "      <td>['hasta', 'que', 'un', 'cabron', 'trata', 'a',...</td>\n",
       "      <td>['cabron', 'trata', 'lacra', 'debe', 'indignan']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>la edad no la pones eh cabron</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>la edad no la pones eh cabron</td>\n",
       "      <td>['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...</td>\n",
       "      <td>['edad', 'no', 'pones', 'eh', 'cabron']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...</td>\n",
       "      <td>['cabron', 'solo', 'puedes', 'obtener', 'cabro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eres mas guiri que los irlandeses de aqui cabr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>eres mas guiri que los irlandeses de aqui cabr...</td>\n",
       "      <td>['eres', 'mas', 'guiri', 'que', 'los', 'irland...</td>\n",
       "      <td>['mas', 'guiri', 'irlandeses', 'aqui', 'cabron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  toxico  num_admir  \\\n",
       "0                                la etiqueta, cabron       1          0   \n",
       "1  hasta que un cabron trata a la lacra como se d...       1          0   \n",
       "2                      la edad no la pones eh cabron       1          0   \n",
       "3      y de un cabron solo puedes obtener cabronadas       1          0   \n",
       "4  eres mas guiri que los irlandeses de aqui cabr...       1          0   \n",
       "\n",
       "   num_may  num_may_esc emojis  num_emoji  num_insultos  dir_23ps  \\\n",
       "0        0          0.0    NaN          0             1         0   \n",
       "1        0          0.0    NaN          0             1         0   \n",
       "2        0          0.0    NaN          0             1         1   \n",
       "3        0          0.0    NaN          0             1         1   \n",
       "4        1          0.1    NaN          0             1         1   \n",
       "\n",
       "   score_emoji_tox  emphasize  \\\n",
       "0              0.0        0.0   \n",
       "1              0.0        0.0   \n",
       "2              0.0        1.0   \n",
       "3              0.0        1.0   \n",
       "4              0.0        1.1   \n",
       "\n",
       "                                       series_token2  \\\n",
       "0                                la etiqueta, cabron   \n",
       "1  hasta que un cabron trata a la lacra como se d...   \n",
       "2                      la edad no la pones eh cabron   \n",
       "3      y de un cabron solo puedes obtener cabronadas   \n",
       "4  eres mas guiri que los irlandeses de aqui cabr...   \n",
       "\n",
       "                                        series_token  \\\n",
       "0                  ['la', 'etiqueta', ',', 'cabron']   \n",
       "1  ['hasta', 'que', 'un', 'cabron', 'trata', 'a',...   \n",
       "2  ['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...   \n",
       "3  ['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...   \n",
       "4  ['eres', 'mas', 'guiri', 'que', 'los', 'irland...   \n",
       "\n",
       "                                      filtered_words  \n",
       "0                            ['etiqueta,', 'cabron']  \n",
       "1   ['cabron', 'trata', 'lacra', 'debe', 'indignan']  \n",
       "2            ['edad', 'no', 'pones', 'eh', 'cabron']  \n",
       "3  ['cabron', 'solo', 'puedes', 'obtener', 'cabro...  \n",
       "4  ['mas', 'guiri', 'irlandeses', 'aqui', 'cabron...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng=df[['texto', 'series_token','num_may_esc','num_insultos','score_emoji_tox','emphasize','toxico']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Redondeo del valor emphasize**\n",
    "\n",
    "Donde 70.55 es 71 y 70.4 es 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(float(1.020408))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_eng.loc[:,'emphasize']=df_eng.loc[:,'emphasize'].map(lambda value: round(float(value),1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>series_token</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>la edad no la pones eh cabron</td>\n",
       "      <td>['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eres mas guiri que los irlandeses de aqui cabr...</td>\n",
       "      <td>['eres', 'mas', 'guiri', 'que', 'los', 'irland...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>que es eso cabron</td>\n",
       "      <td>['que', 'es', 'eso', 'cabron']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alba ‚Äúeste hijo de puta de platini lo hicieron...</td>\n",
       "      <td>['alba', '‚Äú', 'este', 'hijo', 'de', 'puta', 'd...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  \\\n",
       "2                      la edad no la pones eh cabron   \n",
       "3      y de un cabron solo puedes obtener cabronadas   \n",
       "4  eres mas guiri que los irlandeses de aqui cabr...   \n",
       "7                                  que es eso cabron   \n",
       "8  alba ‚Äúeste hijo de puta de platini lo hicieron...   \n",
       "\n",
       "                                        series_token  num_may_esc  \\\n",
       "2  ['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...          0.0   \n",
       "3  ['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...          0.0   \n",
       "4  ['eres', 'mas', 'guiri', 'que', 'los', 'irland...          0.1   \n",
       "7                     ['que', 'es', 'eso', 'cabron']          0.0   \n",
       "8  ['alba', '‚Äú', 'este', 'hijo', 'de', 'puta', 'd...          0.0   \n",
       "\n",
       "   num_insultos  score_emoji_tox  emphasize  toxico  \n",
       "2             1              0.0        1.0       1  \n",
       "3             1              0.0        1.0       1  \n",
       "4             1              0.0        1.1       1  \n",
       "7             1              0.0        1.0       1  \n",
       "8             1              0.0        1.0       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng[df_eng['emphasize']!=0.0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supresi√≥n de abreviaturas en el texto y conservi√≥n a min√∫sculas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario desde el csv que hemos creado\n",
    "path_ab='../00_Dataset/abreviaturas.csv'\n",
    "abrev = pd.read_csv(path_ab, sep=';')\n",
    "diccionario_abr=abrev.set_index('abreviatura').T.to_dict('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parguela vete ya que estabamos contentos pensando que te habias muerto\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parguela vete ya que estabamos contentos pensando que te habias muerto'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creaci√≥n de una funci√≥n en el que palabras como xa se convierten en para\n",
    "def delete_abrev(string):\n",
    "    words=string.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in diccionario_abr.keys():\n",
    "            words[i] = diccionario_abr[words[i]]['significado'].lower()\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "\n",
    "#Comprobaci√≥n del funcionamiento\n",
    "aux=df_eng.iloc[48]['texto']\n",
    "print(aux)\n",
    "delete_abrev(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4384: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "C:\\Users\\Esperanza\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4385: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>series_token</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>la etiqueta, cabron</td>\n",
       "      <td>['la', 'etiqueta', ',', 'cabron']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasta que un cabron trata a la lacra como se d...</td>\n",
       "      <td>['hasta', 'que', 'un', 'cabron', 'trata', 'a',...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>la edad no la pones eh cabron</td>\n",
       "      <td>['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eres mas guiri que los irlandeses de aqui cabr...</td>\n",
       "      <td>['eres', 'mas', 'guiri', 'que', 'los', 'irland...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  \\\n",
       "0                                la etiqueta, cabron   \n",
       "1  hasta que un cabron trata a la lacra como se d...   \n",
       "2                      la edad no la pones eh cabron   \n",
       "3      y de un cabron solo puedes obtener cabronadas   \n",
       "4  eres mas guiri que los irlandeses de aqui cabr...   \n",
       "\n",
       "                                        series_token  num_may_esc  \\\n",
       "0                  ['la', 'etiqueta', ',', 'cabron']          0.0   \n",
       "1  ['hasta', 'que', 'un', 'cabron', 'trata', 'a',...          0.0   \n",
       "2  ['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...          0.0   \n",
       "3  ['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...          0.0   \n",
       "4  ['eres', 'mas', 'guiri', 'que', 'los', 'irland...          0.1   \n",
       "\n",
       "   num_insultos  score_emoji_tox  emphasize  toxico  \n",
       "0             1              0.0        0.0       1  \n",
       "1             1              0.0        0.0       1  \n",
       "2             1              0.0        1.0       1  \n",
       "3             1              0.0        1.0       1  \n",
       "4             1              0.0        1.1       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sustituci√≥n de las abreviaciones en el dataset original\n",
    "df_eng.loc[:,'texto']=df_eng.loc[:,'texto'].map(lambda x: delete_abrev(x))\n",
    "df_eng.is_copy = False\n",
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizaci√≥n quitando las stopwords y los signos de puntuaci√≥n**\n",
    "\n",
    "No siempre se han de quitar todas las stopwords ya que algunas como no si tienen peso en el algoritmo. En este caso eliminamos no del conjunto de palabras vac√≠as y a√±adimos los signos de puntuaci√≥n por tenerlos ya en cuenta en la etiqueta emphasize. \n",
    "\n",
    "Porqu√© eliminar las stopwords siempre no es una buena idea. Fuente: https://medium.com/@wilamelima/why-is-removing-stop-words-not-always-a-good-idea-c8d35bd77214\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asco',\n",
       " 'periodismo',\n",
       " 'cadena',\n",
       " 'perroflauta',\n",
       " 'miserables',\n",
       " 'personajes',\n",
       " 'üò†',\n",
       " 'üò°',\n",
       " 'üò†']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = stopwords.words('spanish')\n",
    "stopWords.remove('no')\n",
    "\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'texto'].map(lambda texto: texto.replace('.',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('?',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('!',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('¬ø',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace(',',''))\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].map(lambda texto: texto.replace('‚Äú',''))\n",
    "#Supresi√≥n de las stopword en castellano\n",
    "df_eng.loc[:,'filtered_words']=df_eng.loc[:,'filtered_words'].apply(lambda frase: [word.lower() for word in frase.split(' ') if word.lower() not in stopWords])\n",
    "\n",
    "df_eng.loc[38,'filtered_words']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmanization**\n",
    "\n",
    "Lemmatizaci√≥n es el proceso de convertir una palabra a su forma b√°sica. La diferencia entre stemming y lemmatization es que la lematizaci√≥n considera el contexto y convierte la palabra a su forma b√°sica significativa, mientras que stemming simplemente elimina los √∫ltimos caracteres, lo que a menudo conduce a significados incorrectos y errores de ortograf√≠a.\n",
    "\n",
    "fuente: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'estudi'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prueba de funcionamiento\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stemmer.stem(\"estudiando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estoy', 'estudiar', 'comer', 'funcionar', 'lemma', 'en', 'espa√±ol', 'y', 'comer', 'no', 'funcionar', '.', 'Pero', 'que', 'decir', 'zampabollos']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "text1 = \"Estoy estudiando como funciona lemma en espa√±ol y como no funciona.Pero que dices zampabollos \"\n",
    "res=[]\n",
    "for token in nlp(text1):\n",
    "    res.append(token.lemma_)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definici√≥n de la funci√≥n que extrae el origen de la palabra\n",
    "def extraccion_lemma(text): \n",
    "    str1 = ' '.join(str(e) for e in text)\n",
    "    res=[]\n",
    "    for word in nlp(str1):\n",
    "        res.append(word.lemma_)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la raiz no es tomada correctamente y no proporciona valor. Pero el lemma funciona suficientemente bien en castellano.  \n",
    "\n",
    "Algunos bugs como los presentados en https://github.com/explosion/spaCy/issues/2710 siguen sin ser solucionados. Palabras como 'estoy' no son traducidas a estar, 'como' es traducido por comer,.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>series_token</th>\n",
       "      <th>num_may_esc</th>\n",
       "      <th>num_insultos</th>\n",
       "      <th>score_emoji_tox</th>\n",
       "      <th>emphasize</th>\n",
       "      <th>toxico</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>filtered_words_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>la etiqueta, cabron</td>\n",
       "      <td>['la', 'etiqueta', ',', 'cabron']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[etiqueta, cabron]</td>\n",
       "      <td>[etiquetar, cabron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasta que un cabron trata a la lacra como se d...</td>\n",
       "      <td>['hasta', 'que', 'un', 'cabron', 'trata', 'a',...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[cabron, trata, lacra, debe, indignan]</td>\n",
       "      <td>[cabron, tratar, lacrar, deber, indignar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>la edad no la pones eh cabron</td>\n",
       "      <td>['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[edad, no, pones, eh, cabron]</td>\n",
       "      <td>[edad, no, poner, eh, cabron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y de un cabron solo puedes obtener cabronadas</td>\n",
       "      <td>['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[cabron, solo, puedes, obtener, cabronadas]</td>\n",
       "      <td>[cabron, solo, poder, obtener, cabronada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eres mas guiri que los irlandeses de aqui cabr...</td>\n",
       "      <td>['eres', 'mas', 'guiri', 'que', 'los', 'irland...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>[mas, guiri, irlandeses, aqui, cabron, xd]</td>\n",
       "      <td>[mas, guiri, irland√©s, aqui, cabron, xd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  \\\n",
       "0                                la etiqueta, cabron   \n",
       "1  hasta que un cabron trata a la lacra como se d...   \n",
       "2                      la edad no la pones eh cabron   \n",
       "3      y de un cabron solo puedes obtener cabronadas   \n",
       "4  eres mas guiri que los irlandeses de aqui cabr...   \n",
       "\n",
       "                                        series_token  num_may_esc  \\\n",
       "0                  ['la', 'etiqueta', ',', 'cabron']          0.0   \n",
       "1  ['hasta', 'que', 'un', 'cabron', 'trata', 'a',...          0.0   \n",
       "2  ['la', 'edad', 'no', 'la', 'pones', 'eh', 'cab...          0.0   \n",
       "3  ['y', 'de', 'un', 'cabron', 'solo', 'puedes', ...          0.0   \n",
       "4  ['eres', 'mas', 'guiri', 'que', 'los', 'irland...          0.1   \n",
       "\n",
       "   num_insultos  score_emoji_tox  emphasize  toxico  \\\n",
       "0             1              0.0        0.0       1   \n",
       "1             1              0.0        0.0       1   \n",
       "2             1              0.0        1.0       1   \n",
       "3             1              0.0        1.0       1   \n",
       "4             1              0.0        1.1       1   \n",
       "\n",
       "                                filtered_words  \\\n",
       "0                           [etiqueta, cabron]   \n",
       "1       [cabron, trata, lacra, debe, indignan]   \n",
       "2                [edad, no, pones, eh, cabron]   \n",
       "3  [cabron, solo, puedes, obtener, cabronadas]   \n",
       "4   [mas, guiri, irlandeses, aqui, cabron, xd]   \n",
       "\n",
       "                         filtered_words_lemm  \n",
       "0                        [etiquetar, cabron]  \n",
       "1  [cabron, tratar, lacrar, deber, indignar]  \n",
       "2              [edad, no, poner, eh, cabron]  \n",
       "3  [cabron, solo, poder, obtener, cabronada]  \n",
       "4   [mas, guiri, irland√©s, aqui, cabron, xd]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng.loc[:,'filtered_words_lemm']=df_eng.loc[:,'filtered_words'].map(lambda text: extraccion_lemma(text) )\n",
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividiendo el dataset en train, dev and test sets \n",
    "Pasos a tener en cuenta: \n",
    "- dev y test deben proceder de la misma distribuci√≥n y se debe tomar de forma aleatoria del total del corpus. \n",
    "- dev y test deben tener el mismo tama√±o. En nuestro caso dividiremos en train 70%, dev 15% y test 15%.\n",
    "- Se quiere tener unos datos no sesgados a la hora de realizar el algoritmo, para ello se cambia el orden de las filas pero inicializando una semilla que fije los resultados dependientes de una variable aleatoria. \n",
    "\n",
    "\n",
    "fuente: https://www.coursera.org/learn/machine-learning-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asco',\n",
       " 'periodismo',\n",
       " 'cadena',\n",
       " 'perroflauta',\n",
       " 'miserables',\n",
       " 'personajes',\n",
       " 'üò†',\n",
       " 'üò°',\n",
       " 'üò†']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng.loc[38,'filtered_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmod=df_eng.reindex(np.random.RandomState(seed=42).permutation(df_eng.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 6) (564,)\n",
      "(141, 6) (141,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filtered_words          [pongo, castigo, cabezabuque]\n",
       "filtered_words_lemm    [poner, castigar, cabezabuque]\n",
       "num_insultos                                        1\n",
       "score_emoji_tox                                     0\n",
       "emphasize                                           1\n",
       "toxico                                              1\n",
       "Name: 372, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "columnas=['filtered_words','filtered_words_lemm','num_insultos', 'score_emoji_tox','emphasize','toxico']\n",
    "dfimp=dfmod.loc[:,columnas]\n",
    "X=dfmod.loc[:,columnas]\n",
    "y=dfmod.loc[:,'toxico']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "#everytime you run it without specifying random_state, you will get a different result\n",
    "print( X_train.shape, y_train.shape) \n",
    "print( X_test.shape, y_test.shape)\n",
    "X_train.iloc[240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado de los datos\n",
    "\n",
    "Pickle es utilizado cuando se tiene un conjunto de datos grande y se carga ese conjunto de datos masivos en la memoria cada vez que ejecuta el programa.Almacenarlo y recuperarlo ser√° mucho m√°s r√°pido, del orden de 50 - 100x, a veces mucho m√°s dependiendo del tama√±o.\n",
    "\n",
    "fuente: https://pythonprogramming.net/python-pickle-module-save-objects-serialization/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asco',\n",
       " 'periodismo',\n",
       " 'cadena',\n",
       " 'perroflauta',\n",
       " 'miserables',\n",
       " 'personajes',\n",
       " 'üò†',\n",
       " 'üò°',\n",
       " 'üò†']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfimp.loc[38,'filtered_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacenamiento del df del modelo\n",
    "pickle_out = open(\"dfmodelo.pickle\",\"wb\")\n",
    "pickle.dump(dfimp, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#Almacenamiento del x_train\n",
    "pickle_out = open(\"X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#Almacenamiento del x_test\n",
    "pickle_out = open(\"X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#Almacenamiento del y_train\n",
    "pickle_out = open(\"y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#Almacenamiento del y_test\n",
    "pickle_out = open(\"y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
